import streamlit as st
import torch
import pickle
import re
from underthesea import word_tokenize
from models.model import EncoderLNRes, DecoderLNRes, Seq2SeqLNRes

# =================== CONFIG ===================
EMB_DIM = 256
HID_DIM = 512
N_LAYERS = 1
DROPOUT = 0.3
MAX_EXTRA_TOKENS = 10  # TƒÉng l√™n ƒë·ªÉ x·ª≠ l√Ω c√¢u d√†i
BEAM_WIDTH = 5  # TƒÉng beam width
LENGTH_PENALTY = 0.7
REPETITION_PENALTY = 1.5  # Th√™m penalty ch·ªëng l·∫∑p
NO_REPEAT_NGRAM_SIZE = 3  # Ch·∫∑n n-gram l·∫∑p

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
st.write(f"‚úÖ Device: {device}")


# =================== TOKENIZER FIXED ===================
def clean_text(text):
    """Gi·ªëng h·ªát v·ªõi training!"""
    # Gi·ªØ l·∫°i . ? ! cho c·∫•u tr√∫c c√¢u
    text = re.sub(r'[^\w\s\.\?\!]', '', text)

    # Chu·∫©n h√≥a s·ªë - GI·ªêNG TRAINING
    text = re.sub(r'\b\d+\b', '<num>', text)

    # Chu·∫©n h√≥a t·ª´ vi·∫øt t·∫Øt th√¥ng d·ª•ng - GI·ªêNG TRAINING
    contractions = {
        "i'm": "im", "i'll": "ill", "i've": "ive",
        "don't": "dont", "can't": "cant", "won't": "wont"
    }
    for cont, replacement in contractions.items():
        text = re.sub(r'\b' + cont + r'\b', replacement, text)

    text = re.sub(r"\s+", " ", text.strip().lower())
    return text


def en_tokenizer(text: str):
    """GI·ªêNG H·ªÜT TRAINING!"""
    return re.findall(r"\b\w+(?:'\w+)?\b", text.lower())


def vi_tokenizer(text: str):
    return word_tokenize(text)


def has_repeated_ngram(sequence, ngram, ngram_size):
    """Ki·ªÉm tra n-gram l·∫∑p trong sequence"""
    if len(sequence) < ngram_size:
        return False
    seq_tuple = tuple(sequence)
    for i in range(len(seq_tuple) - ngram_size + 1):
        if seq_tuple[i:i + ngram_size] == ngram:
            return True
    return False


# =================== TRANSLATE IMPROVED ===================
def translate_sentence_beam_search(model, sentence, vocab_transform, device,
                                   beam_width=BEAM_WIDTH, max_extra=MAX_EXTRA_TOKENS,
                                   length_penalty=LENGTH_PENALTY, repetition_penalty=REPETITION_PENALTY,
                                   no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE):
    """Beam search v·ªõi ch·ªëng l·∫∑p t·ª´"""
    model.eval()

    # CLEAN TEXT GI·ªêNG TRAINING
    cleaned_sentence = clean_text(sentence)
    tokens = en_tokenizer(cleaned_sentence)

    vocab_en = vocab_transform['en']['stoi']
    vocab_vi = vocab_transform['vi']['stoi']
    itos_vi = vocab_transform['vi']['itos']

    bos_en = vocab_en.get("<bos>", vocab_en.get("<sos>"))
    eos_en = vocab_en.get("<eos>", vocab_en.get("</s>"))
    bos_vi = vocab_vi.get("<bos>", vocab_vi.get("<sos>"))
    eos_vi = vocab_vi.get("<eos>", vocab_vi.get("</s>"))
    pad_vi = vocab_vi.get("<pad>")
    unk_vi = vocab_vi.get("<unk>")

    # T·∫°o source sequence GI·ªêNG TRAINING
    src_indices = [vocab_en.get(tok, vocab_en["<unk>"]) for tok in tokens]
    src_indices = [bos_en] + src_indices + [eos_en]
    src_tensor = torch.LongTensor(src_indices).unsqueeze(1).to(device)  # [seq_len, 1]

    with torch.no_grad():
        seq_len = src_tensor.shape[0]
        max_len = min(seq_len + max_extra, 50)  # Gi·ªõi h·∫°n t·ªëi ƒëa

        hidden, cell, _ = model.encoder(src_tensor)
        sequences = [[[], 0.0, hidden, cell]]  # [tokens, score, hidden, cell]

        for step in range(max_len):
            all_candidates = []

            for seq, score, hidden, cell in sequences:
                # N·∫øu sequence ƒë√£ k·∫øt th√∫c, gi·ªØ nguy√™n
                if seq and seq[-1] == eos_vi:
                    all_candidates.append([seq, score, hidden, cell])
                    continue

                input_idx = seq[-1] if seq else bos_vi
                input_tensor = torch.tensor([input_idx]).to(device)
                output, hidden_new, cell_new = model.decoder(input_tensor, hidden, cell)
                log_probs = torch.log_softmax(output, dim=1)

                # üõë √ÅP D·ª§NG REPETITION PENALTY
                if repetition_penalty > 1.0 and seq:
                    for token_id in set(seq[-no_repeat_ngram_size:]):
                        log_probs[0, token_id] /= repetition_penalty

                topk_log_probs, topk_idx = log_probs.topk(beam_width * 2)  # L·∫•y nhi·ªÅu h∆°n ƒë·ªÉ filter

                for i in range(beam_width * 2):
                    token = topk_idx[0, i].item()

                    # üõë CH·∫∂N N-GRAM L·∫∂P
                    if no_repeat_ngram_size > 0 and len(seq) >= no_repeat_ngram_size - 1:
                        ngram = tuple(seq[-(no_repeat_ngram_size - 1):] + [token])
                        if has_repeated_ngram(seq, ngram, no_repeat_ngram_size):
                            continue

                    candidate_seq = seq + [token]
                    new_score = score + topk_log_probs[0, i].item()

                    # Length normalization
                    lp = len(candidate_seq) ** length_penalty
                    length_norm_score = new_score / lp if lp > 0 else new_score

                    all_candidates.append([candidate_seq, length_norm_score, hidden_new, cell_new])

            # Ch·ªçn top k, ∆∞u ti√™n sequence kh√¥ng l·∫∑p
            ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)

            # L·ªçc sequences tr√πng l·∫∑p (theo tokens)
            unique_sequences = []
            seen_tokens = set()
            for seq in ordered:
                seq_tokens = tuple(seq[0])
                if seq_tokens not in seen_tokens:
                    unique_sequences.append(seq)
                    seen_tokens.add(seq_tokens)
                if len(unique_sequences) >= beam_width:
                    break

            sequences = unique_sequences[:beam_width]

            # Stop n·∫øu t·∫•t c·∫£ sequences ƒë·ªÅu k·∫øt th√∫c
            if all(seq[0][-1] == eos_vi for seq in sequences):
                break

        # Ch·ªçn sequence t·ªët nh·∫•t
        best_seq = sequences[0][0]

        # Convert tokens to words, filter special tokens
        translation_tokens = []
        for token_id in best_seq:
            if token_id == eos_vi:
                break
            if token_id not in [bos_vi, eos_vi, pad_vi, unk_vi]:
                translation_tokens.append(itos_vi[token_id])

        return " ".join(translation_tokens) if translation_tokens else "Kh√¥ng th·ªÉ d·ªãch"


def translate_sentence_greedy(model, sentence, vocab_transform, device):
    """Fallback: Greedy decoding ƒë∆°n gi·∫£n"""
    model.eval()

    cleaned_sentence = clean_text(sentence)
    tokens = en_tokenizer(cleaned_sentence)

    vocab_en = vocab_transform['en']['stoi']
    vocab_vi = vocab_transform['vi']['stoi']
    itos_vi = vocab_transform['vi']['itos']

    bos_en = vocab_en.get("<bos>")
    eos_en = vocab_en.get("<eos>")
    bos_vi = vocab_vi.get("<bos>")
    eos_vi = vocab_vi.get("<eos>")

    src_indices = [vocab_en.get(tok, vocab_en["<unk>"]) for tok in tokens]
    src_indices = [bos_en] + src_indices + [eos_en]
    src_tensor = torch.LongTensor(src_indices).unsqueeze(1).to(device)

    with torch.no_grad():
        hidden, cell, _ = model.encoder(src_tensor)

        input_idx = bos_vi
        max_len = len(src_indices) + 10
        translated_tokens = []

        for _ in range(max_len):
            input_tensor = torch.tensor([input_idx]).to(device)
            output, hidden, cell = model.decoder(input_tensor, hidden, cell)

            top1 = output.argmax(1).item()
            if top1 == eos_vi:
                break

            if top1 not in [bos_vi, eos_vi]:
                translated_tokens.append(itos_vi[top1])

            input_idx = top1

        return " ".join(translated_tokens) if translated_tokens else "Kh√¥ng th·ªÉ d·ªãch"


# =================== STREAMLIT APP IMPROVED ===================
st.title("üåç English ‚Üí Vietnamese Translation")
st.write("Nh·∫≠p c√¢u ti·∫øng Anh ƒë·ªÉ d·ªãch sang ti·∫øng Vi·ªát b·∫±ng m√¥ h√¨nh Seq2Seq v·ªõi Beam Search + Ch·ªëng l·∫∑p t·ª´.")

# Sidebar for settings
st.sidebar.header("‚öôÔ∏è C√†i ƒë·∫∑t d·ªãch")
beam_width = st.sidebar.slider("Beam Width", min_value=1, max_value=10, value=BEAM_WIDTH)
use_beam_search = st.sidebar.checkbox("S·ª≠ d·ª•ng Beam Search", value=True)
show_debug = st.sidebar.checkbox("Hi·ªÉn th·ªã th√¥ng tin debug", value=False)

# Load vocab
try:
    with open(r"D:\Qu√¢n\project\translate\data\vocab_transform.pkl", "rb") as f:
        vocab_transform = pickle.load(f)
    st.success("‚úÖ ƒê√£ t·∫£i vocab_transform.pkl")

    if show_debug:
        st.write(f"üìä Vocab sizes ‚Üí EN: {len(vocab_transform['en']['stoi'])} | VI: {len(vocab_transform['vi']['stoi'])}")

except Exception as e:
    st.error(f"‚ùå L·ªói khi t·∫£i vocab_transform.pkl: {e}")
    st.stop()

# Kh·ªüi t·∫°o m√¥ h√¨nh
try:
    enc = EncoderLNRes(len(vocab_transform['en']['stoi']), EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)
    dec = DecoderLNRes(len(vocab_transform['vi']['stoi']), EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)
    model = Seq2SeqLNRes(enc, dec, device).to(device)
except Exception as e:
    st.error(f"‚ùå L·ªói khi kh·ªüi t·∫°o m√¥ h√¨nh: {e}")
    st.stop()

# Load checkpoint
try:
    checkpoint_path = r"D:\Qu√¢n\project\translate\checkpoint\checkpoint_best.pth"
    checkpoint = torch.load(checkpoint_path, map_location=device)

    # X·ª≠ l√Ω c·∫£ hai ƒë·ªãnh d·∫°ng checkpoint
    if "model_state_dict" in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)

    st.success("‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c load th√†nh c√¥ng!")

    if show_debug and "epoch" in checkpoint:
        st.write(f"üìà Checkpoint t·ª´ epoch: {checkpoint['epoch']}")

except Exception as e:
    st.error(f"‚ùå L·ªói khi t·∫£i checkpoint: {e}")
    st.stop()

# Input text
input_text = st.text_area("‚úèÔ∏è Nh·∫≠p c√¢u ti·∫øng Anh:",
                          placeholder="V√≠ d·ª•: Hello, how are you? I'm fine, thank you.",
                          height=100)

if st.button("üöÄ D·ªãch Ngay"):
    if input_text.strip():
        with st.spinner("üîÅ ƒêang d·ªãch..."):
            try:
                if use_beam_search:
                    translation = translate_sentence_beam_search(
                        model, input_text, vocab_transform, device, beam_width=beam_width
                    )
                    method = "Beam Search"
                else:
                    translation = translate_sentence_greedy(model, input_text, vocab_transform, device)
                    method = "Greedy"

                if translation and translation != "Kh√¥ng th·ªÉ d·ªãch":
                    st.success(f"**‚úÖ B·∫£n d·ªãch ({method}):**")
                    st.info(f"**{translation}**")

                    if show_debug:
                        st.write("---")
                        st.write("**üêõ Debug Info:**")
                        st.write(f"- Ph∆∞∆°ng ph√°p: {method}")
                        st.write(f"- Beam width: {beam_width if use_beam_search else 'N/A'}")
                        st.write(f"- Input length: {len(input_text.split())} t·ª´")
                        st.write(f"- Output length: {len(translation.split())} t·ª´")
                else:
                    st.warning("‚ö†Ô∏è Kh√¥ng t·∫°o ƒë∆∞·ª£c b·∫£n d·ªãch. Th·ª≠ c√¢u kh√°c ho·∫∑c ƒëi·ªÅu ch·ªânh c√†i ƒë·∫∑t.")

            except Exception as e:
                st.error(f"‚ùå L·ªói khi d·ªãch: {e}")
                # Fallback to greedy
                st.info("üîÑ Th·ª≠ d√πng Greedy decoding...")
                try:
                    translation = translate_sentence_greedy(model, input_text, vocab_transform, device)
                    if translation:
                        st.success("**‚úÖ B·∫£n d·ªãch (Greedy Fallback):**")
                        st.info(f"**{translation}**")
                except:
                    st.error("‚ùå Kh√¥ng th·ªÉ d·ªãch v·ªõi c·∫£ hai ph∆∞∆°ng ph√°p.")

    else:
        st.warning("‚ùó Vui l√≤ng nh·∫≠p c√¢u ƒë·ªÉ d·ªãch.")

# Example sentences
st.sidebar.header("üìù V√≠ d·ª•")
examples = [
    "Hello, how are you?",
    "I love programming and artificial intelligence.",
    "What is your name?",
    "The weather is beautiful today.",
    "Can you help me with this problem?"
]

for example in examples:
    if st.sidebar.button(example, key=example):
        st.experimental_set_query_params(text=example)
        st.experimental_rerun()

# Check if there's text in URL parameters
query_params = st.experimental_get_query_params()
if "text" in query_params:
    default_text = query_params["text"][0]
else:
    default_text = ""

if default_text:
    st.text_area("‚úèÔ∏è Nh·∫≠p c√¢u ti·∫øng Anh:", value=default_text, height=100)